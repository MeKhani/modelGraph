{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "import numpy as np\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "os.environ['DGLBACKEND'] = \"pytorch\"\n",
    "\n",
    "# Install the CPU version. If you want to install CUDA version, please\n",
    "# refer to https://www.dgl.ai/pages/start.html.\n",
    "device = torch.device(\"cpu\")\n",
    "import dgl\n",
    "import dgl.graphbolt as gb\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is already preprocessed.\n"
     ]
    }
   ],
   "source": [
    "dataset = gb.BuiltinDataset(\"cora-seeds\",root=\"../../../data/datasets\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: link_prediction.\n"
     ]
    }
   ],
   "source": [
    "graph = dataset.graph\n",
    "feature = dataset.feature\n",
    "train_set = dataset.tasks[1].train_set\n",
    "test_set = dataset.tasks[1].test_set\n",
    "task_name = dataset.tasks[1].metadata[\"name\"]\n",
    "print(f\"Task: {task_name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "datapipe = gb.ItemSampler(train_set, batch_size=256, shuffle=True)\n",
    "datapipe = datapipe.sample_uniform_negative(graph, 5)\n",
    "datapipe = datapipe.sample_neighbor(graph, [5, 5, 5])\n",
    "datapipe = datapipe.transform(partial(gb.exclude_seed_edges, include_reverse_edges=True))\n",
    "datapipe = datapipe.fetch_feature(feature, node_feature_keys=[\"feat\"])\n",
    "datapipe = datapipe.copy_to(device)\n",
    "train_dataloader = gb.DataLoader(datapipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniBatch: MiniBatch(seeds=tensor([[2352, 1129],\n",
      "                        [ 893, 2383],\n",
      "                        [1303,  979],\n",
      "                        ...,\n",
      "                        [ 132,   20],\n",
      "                        [ 132, 2046],\n",
      "                        [ 132,   46]], dtype=torch.int32),\n",
      "          sampled_subgraphs=[SampledSubgraphImpl(sampled_csc=CSCFormatBase(indptr=tensor([   0,    2,    6,  ..., 7663, 7664, 7666], dtype=torch.int32),\n",
      "                                                                         indices=tensor([ 144,  931,  144,  ..., 2228, 2250, 2252], dtype=torch.int32),\n",
      "                                                           ),\n",
      "                                               original_row_node_ids=tensor([2352, 1129,  893,  ..., 2342,  594,  663], dtype=torch.int32),\n",
      "                                               original_edge_ids=None,\n",
      "                                               original_column_node_ids=tensor([2352, 1129,  893,  ..., 2337, 2421, 2661], dtype=torch.int32),\n",
      "                            ),\n",
      "                            SampledSubgraphImpl(sampled_csc=CSCFormatBase(indptr=tensor([   0,    2,    6,  ..., 6991, 6996, 6997], dtype=torch.int32),\n",
      "                                                                         indices=tensor([ 144,  931,  144,  ..., 2175, 2371, 1287], dtype=torch.int32),\n",
      "                                                           ),\n",
      "                                               original_row_node_ids=tensor([2352, 1129,  893,  ..., 2337, 2421, 2661], dtype=torch.int32),\n",
      "                                               original_edge_ids=None,\n",
      "                                               original_column_node_ids=tensor([2352, 1129,  893,  ..., 2254, 2018, 1407], dtype=torch.int32),\n",
      "                            ),\n",
      "                            SampledSubgraphImpl(sampled_csc=CSCFormatBase(indptr=tensor([   0,    2,    6,  ..., 3795, 3800, 3802], dtype=torch.int32),\n",
      "                                                                         indices=tensor([ 144,  931,  144,  ..., 1199, 2226, 2071], dtype=torch.int32),\n",
      "                                                           ),\n",
      "                                               original_row_node_ids=tensor([2352, 1129,  893,  ..., 2254, 2018, 1407], dtype=torch.int32),\n",
      "                                               original_edge_ids=None,\n",
      "                                               original_column_node_ids=tensor([2352, 1129,  893,  ..., 2449, 2046,   46], dtype=torch.int32),\n",
      "                            )],\n",
      "          node_features={'feat': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "                                [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "                                [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "                                ...,\n",
      "                                [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "                                [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "                                [0., 0., 0.,  ..., 0., 0., 0.]])},\n",
      "          labels=tensor([1., 1., 1.,  ..., 0., 0., 0.]),\n",
      "          input_nodes=tensor([2352, 1129,  893,  ..., 2342,  594,  663], dtype=torch.int32),\n",
      "          indexes=tensor([  0,   1,   2,  ..., 255, 255, 255]),\n",
      "          edge_features=[{},\n",
      "                        {},\n",
      "                        {}],\n",
      "          compacted_seeds=tensor([[   0,    1],\n",
      "                                  [   2,    3],\n",
      "                                  [   4,    5],\n",
      "                                  ...,\n",
      "                                  [ 423,  757],\n",
      "                                  [ 423, 1289],\n",
      "                                  [ 423, 1290]], dtype=torch.int32),\n",
      "          blocks=[Block(num_src_nodes=2603, num_dst_nodes=2530, num_edges=7666),\n",
      "                 Block(num_src_nodes=2530, num_dst_nodes=2259, num_edges=6997),\n",
      "                 Block(num_src_nodes=2259, num_dst_nodes=1291, num_edges=3802)],\n",
      "       )\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "print(f\"MiniBatch: {data}\")\n",
    "# data.node_pairs_with_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Model for Node Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_size, hidden_size, \"mean\"))\n",
    "        self.layers.append(dglnn.SAGEConv(hidden_size, hidden_size, \"mean\"))\n",
    "        self.hidden_size = hidden_size\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, blocks, x):\n",
    "        hidden_x = x\n",
    "        \n",
    "        for layer_idx, (layer, block) in enumerate(zip(self.layers, blocks)):\n",
    "            hidden_x = layer(block, hidden_x)\n",
    "            is_last_layer = layer_idx == len(self.layers) - 1\n",
    "            if not is_last_layer:\n",
    "                hidden_x = F.relu(hidden_x)\n",
    "        return hidden_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initilize `model` and define `optimizer` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size = feature.size(\"node\",None,\"feat\")[0]\n",
    "model = SAGE(in_size,128).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainig loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93cecc2a75943188258200de7fced88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Loss 0.571\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd63dfda29284915ab571d6437a35bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Loss 0.450\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, data in tqdm(enumerate(train_dataloader)):\n",
    "        # Get node pairs with labels for loss calculation.\n",
    "        compacted_seeds = data.compacted_seeds.T\n",
    "        labels = data.labels\n",
    "        node_feature = data.node_features[\"feat\"]\n",
    "        # Convert sampled subgraphs to DGL blocks.\n",
    "        blocks = data.blocks\n",
    "\n",
    "        # Get the embeddings of the input nodes.\n",
    "        y = model(blocks, node_feature)\n",
    "        logits = model.predictor(\n",
    "            y[compacted_seeds[0]] * y[compacted_seeds[1]]\n",
    "        ).squeeze()\n",
    "\n",
    "        # Compute loss.\n",
    "        loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Loss {total_loss / (step + 1):.3f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dab1aa4c3a43e4bef9eedcd827e266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link Prediction AUC: 0.7427011971878439\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "datapipe = gb.ItemSampler(test_set, batch_size=256, shuffle=False)\n",
    "datapipe = datapipe.copy_to(device)\n",
    "# Since we need to use all neghborhoods for evaluation, we set the fanout\n",
    "# to -1.\n",
    "datapipe = datapipe.sample_neighbor(graph, [-1, -1])\n",
    "datapipe = datapipe.fetch_feature(feature, node_feature_keys=[\"feat\"])\n",
    "eval_dataloader = gb.DataLoader(datapipe, num_workers=0)\n",
    "\n",
    "logits = []\n",
    "labels = []\n",
    "for step, data in tqdm(enumerate(eval_dataloader)):\n",
    "    # Get node pairs with labels for loss calculation.\n",
    "    compacted_seeds = data.compacted_seeds.T\n",
    "    label = data.labels\n",
    "\n",
    "    # The features of sampled nodes.\n",
    "    x = data.node_features[\"feat\"]\n",
    "\n",
    "    # Forward.\n",
    "    y = model(data.blocks, x)\n",
    "    logit = (\n",
    "        model.predictor(y[compacted_seeds[0]] * y[compacted_seeds[1]])\n",
    "        .squeeze()\n",
    "        .detach()\n",
    "    )\n",
    "\n",
    "    logits.append(logit)\n",
    "    labels.append(label)\n",
    "\n",
    "logits = torch.cat(logits, dim=0)\n",
    "labels = torch.cat(labels, dim=0)\n",
    "\n",
    "\n",
    "# Compute the AUROC score.\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(labels.cpu(), logits.cpu())\n",
    "print(\"Link Prediction AUC:\", auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "7\n",
      "0\n",
      "7\n",
      "14\n",
      "1\n",
      "7\n",
      "21\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "x= [1,2,3]\n",
    "y=[7,7,7,7,7,7,7,7,7,7,77,7,7,7]\n",
    "for index, (y1,x1) in enumerate(zip(x, y)):\n",
    "    print(x1)\n",
    "    print(x1*y1)\n",
    "    print(index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
